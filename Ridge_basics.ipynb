{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exploring the Basics of Ridge Regression\n",
    "\n",
    "Ridge regression introduces a penalty term to OLS in order to shrink the coefficient estimates. The motivation behind this is to prevent over fitting to the training data. OLS has low bias because it is fit to the data, but high variance because it will be fit to whatever sample is used. Ridge introduces some bias into the model in order to have less variance across different samples and potentially have better testing performance.\n",
    "\n",
    "### The penalty term\n",
    "\n",
    "Recall that OLS minimizes RSS. Ridge minimizes RSS plus a lambda parameter times the sum of the squares of the coefficients. Here is how that looks:\n",
    "\n",
    "$$\\text{RSS} + \\lambda \\sum_{j=1}^{p} \\beta_j^2$$\n",
    "\n",
    "Note that the intercept is not included in the penalty term and that $\\lambda \\ge 0$.\n",
    "\n",
    "### Lambda\n",
    "This parameter has to be tuned separately. When $\\lambda = 0$, ridge regression is equivalent to OLS. As $\\lambda$ approaches infinity, the coefficients approach 0 which produces a horizontal line that implies no relationship between the independent and dependent variables. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "454e7b0f930540cc"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-10T23:41:11.672182800Z",
     "start_time": "2024-06-10T23:41:10.040669900Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load in Data\n",
    "\n",
    "It is important to note that the inputs for ridge regression are generally standardized. With multiple features of different scales, the penalty would otherwise not have a uniform effect. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c1f0dfc736b4120"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prostate_df = pd.read_pickle('Data/prostate.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-11T00:34:11.153601800Z",
     "start_time": "2024-06-11T00:34:11.148971500Z"
    }
   },
   "id": "9caf781f51c1db5d",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a329ed637fea9fb1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notes:\n",
    "should you standardize one hot encodings (does it only matter for penalized regression?)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a6f8d56dbcbd04b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d60bd79177518a7d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
